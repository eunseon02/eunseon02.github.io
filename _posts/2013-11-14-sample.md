---
layout: post
title: "SLIM: Self-Supervised LiDAR Scene Flow and Motion Segmentation"
categories: [paper review]
tags: [SLIM, scene representation]
description: Sample placeholder post.
---

이 논문은 simultaneous motion segmentation & scene flow estimation using self-supervised training를 위한 논문이다.
### Abstract
**scene flow estimation**을 위한 논문
이전의 연구들은 두 유형의 움직임-독립적으로 움직이는 agents & 센서의 움직임과 일관된 배경의 움직임  -을 self-supervised training 상에서 충분히 고려하지 못했다.

![Untitled](https://github.com/eunseon02/eunseon02.github.io/assets/108911413/78d4793a-83d8-4747-826d-225a3710aad5)


### Introduction
dynamic object의 motion 추정

simultaneous LiDAR scene flow estimation and motion segmentation using a
deep network

- motion segmentation : classification of point into either moving or stationary
- scene flow

### Related Work
**Self-Supervised Learning**

geometric, temporal/cyclic consistency losses 사용

→ predict optical flow, stereo flow, depth,  motion segmentation


### Method
![Untitled (1)](https://github.com/eunseon02/eunseon02.github.io/assets/108911413/9dca53f8-1918-4718-921a-af2d212fbbcb)

### Network Architecture
**Point Cloud Encoder:**
Pillar Feature Net (PFN)을 이용하여 포인트 클라우드를 다음과 같이 Pseudo image형태로 변환하였다.
![image](https://github.com/eunseon02/eunseon02.github.io/assets/108911413/7eb5eb11-3f5c-4a0d-87ae-da7442256a2a)

---

## ***PointPillars: Fast Encoders for Object Detection from Point Clouds***
for 3D Object Detection

using Pillar (x, y에 대한 단위로 나누고 z 축으로는 한계 없이 grouping, z축에 대한 정보는 내부 포인트 클라우드들의 평균 중심점 등 인코딩)

### encoder network

point cloud → sparse pseudo-image 

- $l = (x, y, z, r)$ ($r$: reflection)
- Pillar $(x, y, z, r, x_c, y_c, z_c, x_p, y_p)$
    - $(x_p, y_p)$ : Pillar 중심 좌표로부터의 offset
    - $c$ : pillar 내 모든 포인트들의 산술 평균
    - sparse
- encoding $(D, P, N)$
    - $D$ : pillar의 좌표
    - $P$ : 샘플 당 non-empty pillar의 개수
    - $N$ : 각 pillar당 point의 개수
- Batch Normalization& ReLU$(C, P, N)$
    - $C$ : 각 pillar당 추출된 feature 수
- Max Pooling $(C, P)$
- $(C, H, W)$ : pseudo-image

### **~~2D convolutional backbone~~**

pseudo-image → high-level representation

…

### **~~detection head~~**

…


---
**Flow Backbone:**

**RAFT’s Update Block** in sparsely populated **BEV domain**

update parameter : flow, logit map $\mathcal{L_{cls}}$, $\mathcal{L_{wgt}}$ 

- logit map $\mathcal{L_{cls}}$**(classification)** : output signal to classify the points as stationary or moving **“motion segmentation” ←** global classification threshold $p_{stat}$ (moving averages)
- logit map $\mathcal{L_{wgt}}$ **(confidence)**: **confidence in flow estimate** ← for featureless surfaces flow estimation

- 2D BEV(**Bird's Eye View**) flow map $\mathcal{F}\in \mathbb{R}^{H \times W \times 2}$


**Output Decoder:**

point of input point cloud $\mathcal{P}_t$ : 

flow vector $\textbf{f}_i$ , logit $\mathcal{l_{cls, i}}$(classification), logit $\mathcal{l_{wgt, i}}$(confidence in flow vector)

- rigid-motion odometry transform/aggregated transform $T_r$ → modeling static points & remove background motion
    
    $T_r = \underset{T \in \mathbb{R}^{4 \times 4}}{\argmin} \underset{\textbf{p}_i \in \mathcal{P}_t}{\sum} w_i \| (T - \mathbb{I}_4)\textbf{p}_i - \textbf{f}_i \|^2$
    
    end-to-end train
    
    - 
        
        $\textbf{p}_i \in \mathcal{P}_t$ : 포인트 클라우드 $\mathcal{P}_t$의 모든 점들 $\textbf{p}_i$
        
        $(T - \mathbb{I}_4)\textbf{p}_i - \textbf{f}_i$  : 변환 $T$를 적용한 후 포인트  $\textbf{p}_i$ 와 예측된 flow  $\textbf{f}_i$ 사이의 차이
        
    
- odometry transform
    
    $\left( O_{t \rightarrow t+1}^{-1} - I_4 \right) \textbf{p}_i$
    
    - 
        
        $O_{t \rightarrow t+1}$ : odometry transform - t에서 t+1로 이동하는 동안 차량이나 센서의 변환(transform) -
        
        LiDAR data(3D 포인트 클라우드) $\in \mathbb{R}^{N \times 3}$
        
        image $\in \mathbb{R}^{H \times W \times 2}$ 
        

$T_r^{-1} \approx O_{t \rightarrow t+1}$


**final flow map**

$f_{agg,i} = \begin{cases} (T_r - I_4)\textbf{p}_i & \text{if } l_{cls,i} \geq p_{stat} \\\mathbf{f}_i & \text{if } l_{cls,i} < p_{stat}\end{cases}$
